---
title: '**CLIMATEREADY**'
subtitle: '**Thermal Comfort Survey Amid Heatwaves**'
author: "Juan Gamero-Salinas - Postdoc Researcher @ DATAI University of Navarra"
date: '2024-05-27'
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 4
    theme: united
    keep_md: true
    
---

This R Markdown file allows contains the code for answering the Research Objective 3 (RQ3): **Which factors contribute to an increased likelihood of occupants being clustered into distinct groups of thermal comfort?** of the case study on the research paper **Exploring indoor thermal comfort and its causes and consequences amid heatwaves in a Southern European cityâ€” An unsupervised learning approach** 


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# **K-means**

## **Loading & Inspecting data**

### Set working directory
Set the folder location
```{r echo=FALSE, results='hide',message=FALSE}
setwd("G:/Mi unidad/Post-Doc DATAI/Colaboraciones de investigacion/CLIMATEREADY/encuestas/paper/final draft")
```


### Set the seed for reproducibility
```{r}
set.seed(1)  # Replace 123 with your desired seed value 
```

### Read the csv
```{r}
df <- read.csv("R_logistic_kmeans.csv", sep=";")
head(df, 3)
```


## **Pre-processing data**

### Function to convert TRUE/FALSE to 1/0
```{r}
library(dplyr)

convert_to_binary <- function(x) {
  as.numeric(x)
}
```

###Apply the function to all columns except the first one (assuming it's the index)
```{r}
data_transformed <- df %>%
  mutate_all(~ ifelse(. == "True", 1, ifelse(. == "False", 0, .)))
```


### Print the transformed data
```{r}
head(data_transformed, 3)
names(data_transformed)
length(data_transformed)
nrow(data_transformed)

n = nrow(data_transformed)
n
```




### Define the response variables
```{r}
# Specify the variable to be used as Y response
Y_response <- "cluster_kmeans"

# Extract the Y response
Y <- data_transformed[, Y_response]

# Extract the X response (all other variables)
X <- data_transformed[, setdiff(names(data_transformed), Y_response)]
```


### Identify variables to exclude from conversion to integer
```{r}
exclude_variables <- c("Age", "meanTout")
```


### Convert all other variables to integer
```{r}
X[, setdiff(names(X), exclude_variables)] <- lapply(X[, setdiff(names(X), exclude_variables)], as.integer)
```


### Verify the changes
```{r}
str(X)

X_matrix <- as.matrix(X)
sum(Y)

head(cor(X_matrix),3)
```


### Columns to be removed
For preventing linear dependencies in the input matrix. We modify x so and remove similar columns.
```{r}
columns_to_remove <- c("is_31009", "dwelling_Other")
```


### Index of columns to keep
```{r}
columns_to_keep <- !(colnames(X_matrix) %in% columns_to_remove)
```


### Remove columns
```{r}
X_matrix <- X_matrix[, columns_to_keep]
```


### Scale predictors
```{r}
X_matrix <- scale(X_matrix)
```


## **Analizing data**


### Penalized logistic regression
```{r}
library(glmnet) # Install and load glmnet package
```


```{r}
# Example using cross-validation to find the best lambda with cv = 5
cv_model <- cv.glmnet(X_matrix, Y, alpha = 1, family = "binomial", nfolds = 10, type.measure = "class")
plot(cv_model)

# Find the alpha and lambda with the minimum mean cross-validated error
best_lambda <- cv_model$lambda.min
best_lambda

best_model <- glmnet(X_matrix, Y, alpha = 1, lambda = best_lambda, family = "binomial")
coef(best_model, s = best_lambda)
```



### Selective inference

```{r}
library(selectiveInference)
```



```{r}
# extract coef for a given lambda; note the 1/n factor!
# (and here  we DO  include the intercept term)
beta = coef(best_model, x=X_matrix, y=Y, s = best_lambda/n)
```


```{r}
# compute fixed lambda p-values and selection intervals
model = fixedLassoInf(X_matrix,Y,beta,best_lambda,family="binomial")
model
```





# **Hierarchical Clustering**

## **Loading & Inspecting data**

### Set working directory
Set the folder location
```{r echo=FALSE, results='hide',message=FALSE}
setwd("G:/Mi unidad/Post-Doc DATAI/Colaboraciones de investigacion/CLIMATEREADY/encuestas/paper/final draft")
```

### Set the seed for reproducibility
```{r}
set.seed(1)  # Replace 123 with your desired seed value 
```

### Read the csv
```{r}
df <- read.csv("R_logistic_hclust.csv", sep=";")
head(df, 3)
```


## **Pre-processing data**

### Function to convert TRUE/FALSE to 1/0
```{r}
library(dplyr)

convert_to_binary <- function(x) {
  as.numeric(x)
}
```

###Apply the function to all columns except the first one (assuming it's the index)
```{r}
data_transformed <- df %>%
  mutate_all(~ ifelse(. == "True", 1, ifelse(. == "False", 0, .)))
```


### Print the transformed data
```{r}
head(data_transformed,3)
names(data_transformed)
length(data_transformed)
nrow(data_transformed)

n = nrow(data_transformed)
n
```




### Define the response variables
```{r}
# Specify the variable to be used as Y response
Y_response <- "cluster_hierarchical"

# Extract the Y response
Y <- data_transformed[, Y_response]

# Extract the X response (all other variables)
X <- data_transformed[, setdiff(names(data_transformed), Y_response)]
```


### Identify variables to exclude from conversion to integer
```{r}
exclude_variables <- c("Age", "meanTout")
```


### Convert all other variables to integer
```{r}
X[, setdiff(names(X), exclude_variables)] <- lapply(X[, setdiff(names(X), exclude_variables)], as.integer)
```


### Verify the changes
```{r}
str(X)

X_matrix <- as.matrix(X)
sum(Y) 

head(cor(X_matrix),3)
```


### Columns to be removed
For preventing linear dependencies in the input matrix. We modify x so and remove similar columns.
```{r}
columns_to_remove <- c("is_31009", "dwelling_Other")
```


### Index of columns to keep
```{r}
columns_to_keep <- !(colnames(X_matrix) %in% columns_to_remove)
```


### Remove columns
```{r}
X_matrix <- X_matrix[, columns_to_keep]
```


### Scale predictors
```{r}
X_matrix <- scale(X_matrix)
```


## **Analizing data**


### Penalized logistic regression
```{r}
library(glmnet) # Install and load glmnet package
```


```{r}
# Example using cross-validation to find the best lambda with cv = 5
cv_model <- cv.glmnet(X_matrix, Y, alpha = 1, family = "binomial", nfolds = 10, type.measure = "class")
plot(cv_model)

# Find the alpha and lambda with the minimum mean cross-validated error
best_lambda <- cv_model$lambda.min
best_lambda

best_model <- glmnet(X_matrix, Y, alpha = 1, lambda = best_lambda, family = "binomial")
coef(best_model, s = best_lambda)
```



### Selective inference

```{r}
library(selectiveInference)
```



```{r}
# extract coef for a given lambda; note the 1/n factor!
# (and here  we DO  include the intercept term)
beta = coef(best_model, x=X_matrix, y=Y, s = best_lambda/n)
```


```{r}
# compute fixed lambda p-values and selection intervals
model = fixedLassoInf(X_matrix,Y,beta,best_lambda,family="binomial")
model
```











